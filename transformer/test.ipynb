{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-07T14:23:02.537501Z",
     "start_time": "2025-04-07T14:23:02.533912Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T14:23:14.147750Z",
     "start_time": "2025-04-07T14:23:14.144320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embedding(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, dropout=0.1):\n",
    "        super(Embedding,self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.lut = torch.nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.lut(x)\n",
    "        x = self.dropout(x)\n",
    "        return x * math.sqrt(self.embed_size)"
   ],
   "id": "d51589432f6e8ab",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T14:30:01.275763Z",
     "start_time": "2025-04-07T14:30:01.272220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        # 先创建一个全零矩阵\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        # 用sin cos来填充原来的pos矩阵\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        # 这样我们就得到了位置编码矩阵pe, pe现在还只是一个二维矩阵，要想和embedding的输出（一个三维张量）相加，\n",
    "        # 就必须拓展一个维度，所以这里使用unsqueeze拓展维度.\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n"
   ],
   "id": "848ccd543f72c142",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, dropout=0.1):\n",
    "        super(Encoder,self).__init__()"
   ],
   "id": "abba7133b07309be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
