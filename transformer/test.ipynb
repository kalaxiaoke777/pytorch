{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-08T15:25:27.479013Z",
     "start_time": "2025-04-08T15:25:27.474845Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T15:25:27.487401Z",
     "start_time": "2025-04-08T15:25:27.481928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embedding(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, dropout=0.1):\n",
    "        super(Embedding,self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.lut = torch.nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.lut(x)\n",
    "        x = self.dropout(x)\n",
    "        return x * math.sqrt(self.embed_size)\n",
    "embedding = Embedding(16,512)\n",
    "input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
    "res = embedding(input)"
   ],
   "id": "d51589432f6e8ab",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T15:25:27.502548Z",
     "start_time": "2025-04-08T15:25:27.494355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        # 先创建一个全零矩阵\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        # 用sin cos来填充原来的pos矩阵\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        # 这样我们就得到了位置编码矩阵pe, pe现在还只是一个二维矩阵，要想和embedding的输出（一个三维张量）相加，\n",
    "        # 就必须拓展一个维度，所以这里使用unsqueeze拓展维度.\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "res1 = PositionalEncoding(512, 0.2)\n",
    "res2 = res1(res)"
   ],
   "id": "848ccd543f72c142",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T15:25:27.513113Z",
     "start_time": "2025-04-08T15:25:27.509865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def subsequent_mask(size):\n",
    "    \"\"\"生成向后遮掩的掩码张量, 参数size是掩码张量最后两个维度的大小, 它的最后两维形成一个方阵\"\"\"\n",
    "    # 在函数中, 首先定义掩码张量的形状\n",
    "    attn_shape = (1, size, size)\n",
    "\n",
    "    # 然后使用np.ones方法向这个形状中添加1元素,形成上三角阵, 最后为了节约空间,\n",
    "    # 再使其中的数据类型变为无符号8位整形unit8\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "\n",
    "    # 最后将numpy类型转化为torch中的tensor, 内部做一个1 - 的操作,\n",
    "    # 在这个其实是做了一个三角阵的反转, subsequent_mask中的每个元素都会被1减,\n",
    "    # 如果是0, subsequent_mask中的该位置由0变成1\n",
    "    # 如果是1, subsequent_mask中的该位置由1变成0\n",
    "    return torch.from_numpy(1 - subsequent_mask)\n",
    "mask = subsequent_mask(5)"
   ],
   "id": "abba7133b07309be",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T15:27:19.913869Z",
     "start_time": "2025-04-08T15:27:19.905719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "\n",
    "    dk = query.size(1)\n",
    "\n",
    "    scores = torch.matmul(query,key.transpose(-2, -1)) / math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        #这里为了方便保证后面的softmax等于0 需要将掩码的位置设置为一个-无穷\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    att = torch.nn.functional.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        att = dropout(att)\n",
    "\n",
    "    print(att.shape,value.shape)\n",
    "    return torch.matmul(att, value)\n",
    "query = key = value = res2\n",
    "\n",
    "mask = torch.zeros(2, 4, 4)\n",
    "attn = attention(query, key, value,mask=mask)\n",
    "print(\"attn:\", attn)"
   ],
   "id": "548fe7bc437a6942",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 4]) torch.Size([2, 4, 512])\n",
      "attn: tensor([[[  1.6945,  -6.0764,  18.8077,  ...,   2.0801,  12.3604,  15.9042],\n",
      "         [  1.6945,  -6.0764,  18.8077,  ...,   2.0801,  12.3604,  15.9042],\n",
      "         [  1.6945,  -6.0764,  18.8077,  ...,   2.0801,  12.3604,  15.9042],\n",
      "         [  1.6945,  -6.0764,  18.8077,  ...,   2.0801,  12.3604,  15.9042]],\n",
      "\n",
      "        [[-12.3425,  10.2197,  -2.4821,  ..., -13.9962,   0.9311,  26.9661],\n",
      "         [-12.3425,  10.2197,  -2.4821,  ..., -13.9962,   0.9311,  26.9661],\n",
      "         [-12.3425,  10.2197,  -2.4821,  ..., -13.9962,   0.9311,  26.9661],\n",
      "         [-12.3425,  10.2197,  -2.4821,  ..., -13.9962,   0.9311,  26.9661]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T15:25:27.544568300Z",
     "start_time": "2025-04-08T15:18:21.795879Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a901c8c2e7798a24",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[55], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m query \u001B[38;5;241m=\u001B[39m key \u001B[38;5;241m=\u001B[39m value \u001B[38;5;241m=\u001B[39m res2\n\u001B[1;32m----> 2\u001B[0m attn, \u001B[38;5;241m=\u001B[39m attention(query, key, value, mask\u001B[38;5;241m=\u001B[39mmask)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattn:\u001B[39m\u001B[38;5;124m\"\u001B[39m, attn)\n",
      "Cell \u001B[1;32mIn[54], line 9\u001B[0m, in \u001B[0;36mattention\u001B[1;34m(query, key, value, mask, dropout)\u001B[0m\n\u001B[0;32m      5\u001B[0m scores \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmatmul(query,key\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)) \u001B[38;5;241m/\u001B[39m math\u001B[38;5;241m.\u001B[39msqrt(dk)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;66;03m#这里为了方便保证后面的softmax等于0 需要将掩码的位置设置为一个-无穷\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m     scores \u001B[38;5;241m=\u001B[39m scores\u001B[38;5;241m.\u001B[39mmasked_fill(mask \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1e9\u001B[39m)\n\u001B[0;32m     10\u001B[0m att \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msoftmax(scores, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dropout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 2"
     ]
    }
   ],
   "execution_count": 55
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
