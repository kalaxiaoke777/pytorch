{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-09T07:49:18.072794Z",
     "start_time": "2025-04-09T07:49:18.067006Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T07:49:18.104312Z",
     "start_time": "2025-04-09T07:49:18.087397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embedding(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, dropout=0.1):\n",
    "        super(Embedding,self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.lut = torch.nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.lut(x)\n",
    "        x = self.dropout(x)\n",
    "        return x * math.sqrt(self.embed_size)\n",
    "embedding = Embedding(16,512)\n",
    "input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
    "res = embedding(input)"
   ],
   "id": "d51589432f6e8ab",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T07:49:18.386862Z",
     "start_time": "2025-04-09T07:49:18.319626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        # 先创建一个全零矩阵\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        # 用sin cos来填充原来的pos矩阵\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        # 这样我们就得到了位置编码矩阵pe, pe现在还只是一个二维矩阵，要想和embedding的输出（一个三维张量）相加，\n",
    "        # 就必须拓展一个维度，所以这里使用unsqueeze拓展维度.\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "res1 = PositionalEncoding(512, 0.2)\n",
    "res2 = res1(res)"
   ],
   "id": "848ccd543f72c142",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T07:49:18.527581Z",
     "start_time": "2025-04-09T07:49:18.470872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def subsequent_mask(size):\n",
    "    \"\"\"生成向后遮掩的掩码张量, 参数size是掩码张量最后两个维度的大小, 它的最后两维形成一个方阵\"\"\"\n",
    "    # 在函数中, 首先定义掩码张量的形状\n",
    "    attn_shape = (1, size, size)\n",
    "\n",
    "    # 然后使用np.ones方法向这个形状中添加1元素,形成上三角阵, 最后为了节约空间,\n",
    "    # 再使其中的数据类型变为无符号8位整形unit8\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "\n",
    "    # 最后将numpy类型转化为torch中的tensor, 内部做一个1 - 的操作,\n",
    "    # 在这个其实是做了一个三角阵的反转, subsequent_mask中的每个元素都会被1减,\n",
    "    # 如果是0, subsequent_mask中的该位置由0变成1\n",
    "    # 如果是1, subsequent_mask中的该位置由1变成0\n",
    "    return torch.from_numpy(1 - subsequent_mask)\n",
    "mask = subsequent_mask(5)"
   ],
   "id": "abba7133b07309be",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T07:49:18.595289Z",
     "start_time": "2025-04-09T07:49:18.552851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "\n",
    "    dk = query.size(1)\n",
    "\n",
    "    scores = torch.matmul(query,key.transpose(-2, -1)) / math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        #这里为了方便保证后面的softmax等于0 需要将掩码的位置设置为一个-无穷\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    att = torch.nn.functional.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        att = dropout(att)\n",
    "\n",
    "    print(att.shape,value.shape)\n",
    "\n",
    "    return torch.matmul(att, value)\n",
    "\n",
    "query = key = value = res2\n",
    "\n",
    "mask = torch.zeros(2, 4, 4)\n",
    "attn = attention(query, key, value,mask=mask)\n",
    "print(\"attn:\", attn)"
   ],
   "id": "548fe7bc437a6942",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 4]) torch.Size([2, 4, 512])\n",
      "attn: tensor([[[ -5.6566,   3.1390,  -3.0825,  ...,  -6.0775,  -8.8436,  -1.9522],\n",
      "         [ -5.6566,   3.1390,  -3.0825,  ...,  -6.0775,  -8.8436,  -1.9522],\n",
      "         [ -5.6566,   3.1390,  -3.0825,  ...,  -6.0775,  -8.8436,  -1.9522],\n",
      "         [ -5.6566,   3.1390,  -3.0825,  ...,  -6.0775,  -8.8436,  -1.9522]],\n",
      "\n",
      "        [[  5.9229, -20.5859,  -0.6315,  ...,   7.3740, -10.7739,  13.3334],\n",
      "         [  5.9229, -20.5859,  -0.6315,  ...,   7.3740, -10.7739,  13.3334],\n",
      "         [  5.9229, -20.5859,  -0.6315,  ...,   7.3740, -10.7739,  13.3334],\n",
      "         [  5.9229, -20.5859,  -0.6315,  ...,   7.3740, -10.7739,  13.3334]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T07:51:15.265608Z",
     "start_time": "2025-04-09T07:51:15.250079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "def clone_linear(linear,size):\n",
    "    return torch.nn.ModuleList([copy.deepcopy(linear) for _ in range(size)])\n",
    "class MutiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self,heads,embed_size,dropout=0.1):\n",
    "        super(MutiHeadAttention, self).__init__()\n",
    "        self.heads = heads\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        assert embed_size % heads == 0\n",
    "        self.d_k = embed_size // heads\n",
    "\n",
    "        self.linear = clone_linear(torch.nn.Linear(embed_size,embed_size),4)\n",
    "\n",
    "        self.attn = None\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    def forward(self,query,key,value,mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(0)\n",
    "        # 样本个数\n",
    "        batch_size = query.size(0)\n",
    "        # 切割多头，然后矩阵变换到，切割的次数\n",
    "        # 这里只所有需要进行seq,self.heads的转，是因为，heads其实并不重要，seq可以放在后面，这样一seq和dk就可以在一个头中矩阵相乘\n",
    "        # 论文中的解释就是效果好\n",
    "        # 将Q K V 扔到model的线性层\n",
    "        query,key,value = [model(x).view(batch_size,-1,self.heads,self.d_k).transpose(1,2)  for model,x in zip(self.linear,(query,key,value))]\n",
    "        # 注意力机制\n",
    "        x = attention(query,key,value,mask,self.dropout)\n",
    "\n",
    "\n",
    "        # 逆向操作，回到三维张量\n",
    "        x = x.transpose(1,2).contiguous().view(batch_size,-1,self.heads * self.d_k)\n",
    "\n",
    "\n",
    "        # 这里再使用最后一个线性变换方法进行线性变换\n",
    "        return  self.linear[-1](x)\n"
   ],
   "id": "22c5364ccc157dff",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T07:51:27.888007Z",
     "start_time": "2025-04-09T07:51:27.852988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "head = 8\n",
    "embedding = 512\n",
    "query = key = value = res2\n",
    "mutiHeadAttention = MutiHeadAttention(heads=head,embed_size=embedding,dropout=0.1)\n",
    "mask = torch.zeros([8,4,4])\n",
    "x = mutiHeadAttention(query,key,value,mask=mask)"
   ],
   "id": "49f37ffeba30f39a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4, 4]) torch.Size([2, 8, 4, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:44:19.903096Z",
     "start_time": "2025-04-09T08:44:19.786050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionWiseFeedForward(torch.nn.Module):\n",
    "    def __init__(self,d_model,d_ff,dropout=0.1):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(d_model,d_ff)\n",
    "        self.fc2 = torch.nn.Linear(d_ff,d_model)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(self.dropout(x))\n",
    "        return x\n",
    "positionWiseFeedForward = PositionWiseFeedForward(512,512*4,dropout=0.1)\n",
    "wise_res = positionWiseFeedForward(x)"
   ],
   "id": "8b87ed5ac0dcb2d1",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:43.223136Z",
     "start_time": "2025-04-09T08:48:43.200229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super(LayerNorm,self).__init__()\n",
    "\n",
    "        # 定义一个a b 参数并需要训练\n",
    "        self.a_1 = torch.nn.Parameter(torch.ones(d_model))\n",
    "        self.b_1 = torch.nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "\n",
    "        return  self.a_1 * (x - mean) / (std + self.eps) + self.b_1\n",
    "\n",
    "features = d_model = 512\n",
    "eps = 1e-6\n",
    "layerNorm = LayerNorm(features)\n",
    "x = layerNorm(wise_res)"
   ],
   "id": "91651599dcb0fc72",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2281, -0.6313,  0.4876,  ...,  0.3830,  0.0751,  1.7192],\n",
       "         [ 0.0059,  0.8614, -0.3627,  ..., -0.1831,  0.5181,  0.0025],\n",
       "         [ 0.2073, -0.4178,  0.2634,  ...,  0.4646, -0.8108,  1.4696],\n",
       "         [-0.3626,  0.3036,  0.5932,  ...,  0.5363, -0.1390,  0.3032]],\n",
       "\n",
       "        [[-0.5965, -0.8341, -0.3698,  ...,  0.6870, -0.2825,  1.7752],\n",
       "         [-0.5176, -0.6484,  0.1244,  ...,  0.6189,  0.4491,  1.5456],\n",
       "         [-0.1680, -1.9283,  0.1998,  ...,  0.4091, -0.1784,  1.4479],\n",
       "         [-0.5876, -0.5583, -0.2078,  ..., -0.2927, -0.5400,  1.8870]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
