{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-10T14:10:31.534709Z",
     "start_time": "2025-04-10T14:10:31.527053Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from numpy.lib.utils import source\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:10:31.555793Z",
     "start_time": "2025-04-10T14:10:31.553262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, dropout=0.1):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.embedding(x))"
   ],
   "id": "22a5a2f1a8ae8358",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:10:31.574309Z",
     "start_time": "2025-04-10T14:10:31.570499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = 16\n",
    "embed_size = 512\n",
    "input = torch.LongTensor([[1,3,4,5],[2,4,5,6],[3,5,6,7]])\n",
    "embedding = Embedding(vocab_size,embed_size)\n",
    "embedding_res = embedding(input)\n",
    "embedding_res.shape"
   ],
   "id": "1009b88801e6ee1b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:10:31.583547Z",
     "start_time": "2025-04-10T14:10:31.579172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "\n",
    "        positions = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "\n",
    "        self.pe[:, 0::2] = torch.sin(positions * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(positions * div_term)\n",
    "\n",
    "\n",
    "        self.pe = self.pe.unsqueeze(0)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)"
   ],
   "id": "e2caa8529be3219a",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:10:31.607614Z",
     "start_time": "2025-04-10T14:10:31.598977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d_model = 512\n",
    "positionalEncoding = PositionalEncoding(d_model)\n",
    "pe_res = positionalEncoding(embedding_res)\n",
    "query = key = value = pe_res"
   ],
   "id": "ac8acafe59de949d",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:10:31.632162Z",
     "start_time": "2025-04-10T14:10:31.619303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "\n",
    "    d_k = query.size(1)\n",
    "    source = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        source = source.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    att = F.softmax(source, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        att = dropout(att)\n",
    "\n",
    "    return torch.matmul(att, value)\n",
    "\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head, embed_size, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.head = head\n",
    "        self.embed_size = embed_size\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        assert d_model % head == 0\n",
    "        self.d_k = embed_size // head\n",
    "\n",
    "        self.linear = clones(nn.Linear(embed_size, embed_size), 4)\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "\n",
    "        batch_size = query.size(0)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(0)\n",
    "        query, key, value = [model(x).view(batch_size,-1,self.head,self.d_k).transpose(1,2) for model, x in zip(self.linear,(query, key, value))]\n",
    "\n",
    "        x = attention(query, key, value, mask=mask, dropout=dropout)\n",
    "        x = x.transpose(1,2).contiguous().view(batch_size, -1, self.head*self.d_k)\n",
    "\n",
    "        return self.linear[-1](x)\n",
    "head = 8\n",
    "embed_size = 512\n",
    "multiHeadAttention = MultiHeadAttention(head, embed_size, dropout=0.1)\n",
    "mask = torch.zeros([8,4,4])\n",
    "x = multiHeadAttention(query, key, value, mask)"
   ],
   "id": "f52b3ce4a7e308ca",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:10:31.647587Z",
     "start_time": "2025-04-10T14:10:31.644468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "        return x"
   ],
   "id": "3ef1be401e6c5914",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:10:31.668279Z",
     "start_time": "2025-04-10T14:10:31.658813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "positionwiseFeedForward = PositionwiseFeedForward(512,512*4,dropout=0.1)\n",
    "x = positionwiseFeedForward(x)"
   ],
   "id": "ef7ed31ab4b18d29",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:10:31.682549Z",
     "start_time": "2025-04-10T14:10:31.679891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ],
   "id": "fea77722eeee41ee",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:10:31.707584Z",
     "start_time": "2025-04-10T14:10:31.703764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # 这里是将位置编码结果和注意力进行残差连接\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        # 然后再残差连接至前馈神经层\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ],
   "id": "9db0f95a805662be",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:10:31.715060Z",
     "start_time": "2025-04-10T14:10:31.711471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = nn.LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            # 这里会进行n次连接\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ],
   "id": "d36cd030a1dc2a05",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:27:39.570484Z",
     "start_time": "2025-04-10T14:27:39.517458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, d_model, d_ff, head, N, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embed_size, dropout)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        self.encoder = Encoder(\n",
    "            EncoderLayer(\n",
    "                size=d_model,\n",
    "                self_attn=MultiHeadAttention(head, d_model, dropout),\n",
    "                feed_forward=PositionwiseFeedForward(d_model, d_ff, dropout),\n",
    "                dropout=dropout\n",
    "            ),\n",
    "            N\n",
    "        )\n",
    "\n",
    "    def forward(self, src, mask):\n",
    "        x = self.embedding(src)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.encoder(x, mask)\n",
    "        return x\n",
    "\n",
    "vocab_size = 16\n",
    "embed_size = 512\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "head = 8\n",
    "N = 6  # 编码器层数\n",
    "def make_mask(src, pad_token=0):\n",
    "    mask = (src != pad_token).unsqueeze(-2)\n",
    "    return mask\n",
    "model = TransformerEncoder(vocab_size, embed_size, d_model, d_ff, head, N)\n",
    "\n",
    "src = torch.LongTensor([[1, 3, 4, 5], [2, 4, 5, 6], [3, 5, 6, 7]])\n",
    "mask = torch.zeros([8, 4, 4])\n",
    "output = model(src, mask)\n",
    "output.size(),output.shape"
   ],
   "id": "405cf8bef2a309d5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4, 512]), torch.Size([3, 4, 512]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:27:41.424003Z",
     "start_time": "2025-04-10T14:27:41.418220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用DecoderLayer的类实现解码器层\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        \"\"\"初始化函数的参数有5个, 分别是size，代表词嵌入的维度大小, 同时也代表解码器层的尺寸，\n",
    "            第二个是self_attn，多头自注意力对象，也就是说这个注意力机制需要Q=K=V，\n",
    "            第三个是src_attn，多头注意力对象，这里Q!=K=V， 第四个是前馈全连接层对象，最后就是droupout置0比率.\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # 在初始化函数中， 主要就是将这些输入传到类中\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        # 按照结构图使用clones函数克隆三个子层连接对象.dsa\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, source_mask, target_mask):\n",
    "        \"\"\"forward函数中的参数有4个，分别是来自上一层的输入x，\n",
    "           来自编码器层的语义存储变量mermory， 以及源数据掩码张量和目标数据掩码张量.\n",
    "        \"\"\"\n",
    "        # 将memory表示成m方便之后使用\n",
    "        m = memory\n",
    "\n",
    "        # 将x传入第一个子层结构，第一个子层结构的输入分别是x和self-attn函数，因为是自注意力机制，所以Q,K,V都是x，\n",
    "        # 最后一个参数是目标数据掩码张量，这时要对目标数据进行遮掩，因为此时模型可能还没有生成任何目标数据，\n",
    "        # 比如在解码器准备生成第一个字符或词汇时，我们其实已经传入了第一个字符以便计算损失，\n",
    "        # 但是我们不希望在生成第一个字符时模型能利用这个信息，因此我们会将其遮掩，同样生成第二个字符或词汇时，\n",
    "        # 模型只能使用第一个字符或词汇信息，第二个字符以及之后的信息都不允许被模型使用.\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, target_mask))\n",
    "\n",
    "        # 接着进入第二个子层，这个子层中常规的注意力机制，q是输入x; k，v是编码层输出memory，\n",
    "        # 同样也传入source_mask，但是进行源数据遮掩的原因并非是抑制信息泄漏，而是遮蔽掉对结果没有意义的字符而产生的注意力值，\n",
    "        # 以此提升模型效果和训练速度. 这样就完成了第二个子层的处理.\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, source_mask))\n",
    "\n",
    "        # 最后一个子层就是前馈全连接子层，经过它的处理后就可以返回结果.这就是我们的解码器层结构.\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ],
   "id": "65b60d6dd2521c8d",
   "outputs": [],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
