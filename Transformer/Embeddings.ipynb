{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-07T08:59:10.940285Z",
     "start_time": "2025-04-07T08:59:10.932798Z"
    }
   },
   "source": [
    "# 导入必备的工具包\n",
    "import torch\n",
    "\n",
    "# 预定义的网络层torch.nn, 工具开发者已经帮助我们开发好的一些常用层,\n",
    "# 比如，卷积层, lstm层, embedding层等, 不需要我们再重新造轮子.\n",
    "import torch.nn as nn\n",
    "\n",
    "# 数学计算工具包\n",
    "import math\n",
    "\n",
    "# torch中变量封装函数Variable.\n",
    "from torch.autograd import Variable"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:59:10.976484Z",
     "start_time": "2025-04-07T08:59:10.968214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embadding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(Embadding, self).__init__()\n",
    "\n",
    "        self.lut = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embed_size = embed_size\n",
    "    def forward(self, input):\n",
    "        return self.lut(input) * math.sqrt(self.embed_size)"
   ],
   "id": "201c409ddbb7a0ef",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:59:11.053242Z",
     "start_time": "2025-04-07T08:59:11.036746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embadding = Embadding(1000,128)\n",
    "# 这个不能超过vocab_size\n",
    "input = torch.LongTensor([[1,2,4,5],[4,3,2,9],[1,4,999,9]])\n",
    "x = embadding(input)"
   ],
   "id": "8bd0d7f7e3a8a0c7",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T09:06:04.470313Z",
     "start_time": "2025-04-07T09:06:04.439833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        \"\"\"\n",
    "        embed_dim: 嵌入维度\n",
    "        max_len: 序列的最大长度\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # 创建位置编码矩阵\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) *\n",
    "                             -(math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, input):\n",
    "        return self.dropout(input + Variable(self.pe[:, :input.size(1)],requires_grad=False))\n",
    "ps = PositionalEncoding(128)\n",
    "ps(x),x"
   ],
   "id": "cf6641d0d16d0ed3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ -0.0000,  28.7924,  -0.0000,  ...,   0.0000, -17.9705,  -6.7543],\n",
       "          [ 17.8949,  -0.0000,  12.1308,  ...,  10.2210, -14.2175,  -0.0000],\n",
       "          [  2.3334,  15.9196,   6.3648,  ...,   0.5610,  -0.0000,  -0.0000],\n",
       "          [-17.1841, -12.2876, -12.9305,  ...,  -5.4904, -19.3377,   0.0000]],\n",
       " \n",
       "         [[  1.1968,  17.6898,   5.1310,  ...,   0.5610,  -8.2305, -11.1611],\n",
       "          [ 12.6418,  -3.4836,  -4.0453,  ...,   5.2866,   8.0946, -12.1409],\n",
       "          [ 17.9797,  -0.0000,  12.4125,  ...,  10.2210, -14.2174,  -9.5930],\n",
       "          [ 11.4116,  -3.1542,  -9.1871,  ...,  29.8916,   0.0000,  -3.0731]],\n",
       " \n",
       "         [[ -9.1296,   0.0000,  -4.7802,  ...,  13.1341, -17.9705,  -6.7543],\n",
       "          [  2.2486,  17.1152,   6.0831,  ...,   0.0000,  -8.2304,  -0.0000],\n",
       "          [ 12.2383,  -8.3959,   0.0000,  ...,  20.3719, -10.3864,  -6.4068],\n",
       "          [ 11.4116,  -0.0000,  -0.0000,  ...,  29.8916,   1.0979,  -3.0731]]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([[[ -7.3037,  22.0340,  -3.8242,  ...,   9.5073, -14.3764,  -6.4035],\n",
       "          [ 13.4745, -22.1202,   8.9429,  ...,   7.1768, -11.3741,  -8.6744],\n",
       "          [  0.9574,  13.1519,   4.1048,  ...,  -0.5512,  -6.5844,  -9.9289],\n",
       "          [-13.8884,  -8.8401, -10.8617,  ...,  -5.3923, -15.4705,  24.9598]],\n",
       " \n",
       "         [[  0.9574,  13.1519,   4.1048,  ...,  -0.5512,  -6.5844,  -9.9289],\n",
       "          [  9.2719,  -3.3272,  -3.9979,  ...,   3.2292,   6.4756, -10.7127],\n",
       "          [ 13.4745, -22.1202,   8.9429,  ...,   7.1768, -11.3741,  -8.6744],\n",
       "          [  8.9881,  -1.5333,  -7.8670,  ...,  22.9132,   0.8780,  -3.4585]],\n",
       " \n",
       "         [[ -7.3037,  22.0340,  -3.8242,  ...,   9.5073, -14.3764,  -6.4035],\n",
       "          [  0.9574,  13.1519,   4.1048,  ...,  -0.5512,  -6.5844,  -9.9289],\n",
       "          [  8.8813,  -6.3006,   0.1067,  ...,  15.2975,  -8.3094,  -6.1254],\n",
       "          [  8.9881,  -1.5333,  -7.8670,  ...,  22.9132,   0.8780,  -3.4585]]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
