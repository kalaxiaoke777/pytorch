{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-21T07:12:25.970370Z",
     "start_time": "2025-04-21T07:12:25.963388Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import jieba"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T08:01:32.255418Z",
     "start_time": "2025-04-21T08:01:31.852496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "content = pd.read_csv('../data/train.txt', sep='\\t')\n",
    "print(content.head())\n",
    "count = Counter(content.label.values)\n"
   ],
   "id": "4069ef13fb3d0302",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    sentence  label\n",
      "0         中华女子学院：本科层次仅1专业招男生      3\n",
      "1     两天价网站背后重重迷雾：做个网站究竟要多少钱      4\n",
      "2  东5环海棠公社230-290平2居准现房98折优惠      1\n",
      "3  卡佩罗：告诉你德国脚生猛的原因 不希望英德战踢点球      7\n",
      "4    82岁老太为学生做饭扫地44年获授港大荣誉院士      5\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T07:12:49.930385Z",
     "start_time": "2025-04-21T07:12:26.396381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 分词处理\n",
    "def cut_sentence(sentence):\n",
    "    sentence = jieba.cut(sentence)\n",
    "    return list(sentence)\n",
    "content['words'] = content['sentence'].apply(cut_sentence)"
   ],
   "id": "ddb9c1a1d3bccedd",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T07:12:51.243044Z",
     "start_time": "2025-04-21T07:12:49.943911Z"
    }
   },
   "cell_type": "code",
   "source": "content.to_csv('../data/train_new.csv')",
   "id": "9826417b54169ddb",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T07:12:51.261701Z",
     "start_time": "2025-04-21T07:12:51.254612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from icecream import ic\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score"
   ],
   "id": "f8d37b5db0a54e56",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T07:12:51.284061Z",
     "start_time": "2025-04-21T07:12:51.279027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TRAIN_CORPUS = '../data/train_new.csv'\n",
    "STOP_WORDS = '../data/stopwords.txt'\n",
    "WORDS_COLUMN = 'words'"
   ],
   "id": "de156d9b25fddcb1",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T07:12:52.246821Z",
     "start_time": "2025-04-21T07:12:51.299886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "content = pd.read_csv(TRAIN_CORPUS)\n",
    "corpus = content[WORDS_COLUMN].values"
   ],
   "id": "73b249f76d1a9423",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T07:12:52.337932Z",
     "start_time": "2025-04-21T07:12:52.264395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total = 0\n",
    "for i, v in count.items():\n",
    "    total += v\n",
    "\n",
    "print(total)\n",
    "\n",
    "for i, v in count.items():\n",
    "    print(i, v / total * 100, '%')\n",
    "\n",
    "content['sentence_len'] = content['sentence'].apply(len)\n",
    "length_mean = np.mean(content['sentence_len'])\n",
    "length_std = np.std(content['sentence_len'])\n",
    "length_mean,length_std"
   ],
   "id": "307ae4ff7f7ff78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000\n",
      "3 10.0 %\n",
      "4 10.0 %\n",
      "1 10.0 %\n",
      "7 10.0 %\n",
      "5 10.0 %\n",
      "9 10.0 %\n",
      "8 10.0 %\n",
      "2 10.0 %\n",
      "6 10.0 %\n",
      "0 10.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(19.21257222222222), np.float64(3.8637872533601523))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%%sql\n",
   "id": "e481feb3bd3f2f03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T08:01:35.929610Z",
     "start_time": "2025-04-21T08:01:35.852974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stop_words_size = 749\n",
    "WORDS_LONG_TAIL_BEGIN = 10000\n",
    "WORDS_SIZE = WORDS_LONG_TAIL_BEGIN - stop_words_size\n",
    "\n",
    "stop_words = open(STOP_WORDS).read().split()[:stop_words_size]"
   ],
   "id": "9e5e859124e00840",
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0x9a in position 4: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[56], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m WORDS_LONG_TAIL_BEGIN \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10000\u001B[39m\n\u001B[0;32m      3\u001B[0m WORDS_SIZE \u001B[38;5;241m=\u001B[39m WORDS_LONG_TAIL_BEGIN \u001B[38;5;241m-\u001B[39m stop_words_size\n\u001B[1;32m----> 5\u001B[0m stop_words \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(STOP_WORDS)\u001B[38;5;241m.\u001B[39mread()\u001B[38;5;241m.\u001B[39msplit()[:stop_words_size]\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'gbk' codec can't decode byte 0x9a in position 4: illegal multibyte sequence"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-21T08:01:46.074923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count = 0\n",
    "train_data = []\n",
    "with open('../data/train.txt', 'r', encoding='utf-8') as f2:\n",
    "    for line in f2.readlines():\n",
    "        line = line.strip('\\n').strip()\n",
    "        sentence, label = line.split('\\t')\n",
    "\n",
    "        # 1: 首先处理标签部分\n",
    "        label_id = int(label)\n",
    "        label_name = id_to_label[label_id]\n",
    "        new_label = '__label__' + label_name\n",
    "\n",
    "        # 2: 然后处理文本部分, 为了便于后续增加n-gram特性, 可以按字划分, 也可以按词划分\n",
    "        sent_char = ' '.join(list(sentence))\n",
    "\n",
    "        # 3: 将文本和标签组合成fasttext规定的格式\n",
    "        new_sentence = new_label + ' ' + sent_char\n",
    "        train_data.append(new_sentence)\n",
    "\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print('count=', count)"
   ],
   "id": "1174268fd6787929",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "392c2465ffe82c09"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
